\section{Metodologia experimental}

Particionou-se a base de dados utilizando-se a metodologia de validação cruzada \emph{(k-fold cross-validation)}, visto que os dados não são sensíveis ao tempo. Utilizou-se 10 partições, sendo 9 delas para o treinamento e 1 para a validação, dessa forma os conjuntos de treinamento contém 40653 amostras e os conjuntos de teste 4517 escolhidas aleatóriamente.

Para avaliação do poder de classificação de cada método aplicou-se as medidas mais utilizadas, como acurácia, F-medida, precisão e revocação, contabilizando também o tempo de treinamento e teste de cada partição.

A fim de verificar a possibilidade de superajustamento ou subajustamento, gerou-se também os gráficos das curvas de aprendizando, realizando os treinamento com partições incrementais, iniciando com 1 partição e finalizando com 9.

Apresenta-se aqui os parâmetros selecionados, a fim de possibilitar a reprodução dos resultados obtidos em cada método:

\subsection{KNN}

O KNN \emph{(K-vizinhos mais próximos)} é um método baseado em distâncias que consiste em selecionar os K vizinhos do conjunto de treinamento menos distante da amostra de teste, e por distante entende-se, que apresente a menor diferença entre os atributos.

O único parâmetro do KNN é o valor K, para o qual testou-se com os valores: 1, 3, 5, 7, 11, 21, 51. 

\subsection{Regressão logística}

O método da regressão logística consiste em encontrar uma função \emph{(hipótese)} que classifique os atributos, minimizando o erro entre as amostras, através do ajuste dos coeficientes do polinômio \(\theta\).

Implementou-se 3 variações das hipóteses:

\begin{description}
\item[Hipótese Linear] \hfill \\ Atributos elevados a primeira potência;
\item[Hipótese Quadrática] \hfill \\ Atributos elevados a primeira e segunda potência;
\item[Hipótese Cúbica] \hfill \\ Atributos elevados a primeira, segunda e terceira potência;
\end{description}

A regressão logística ainda pode utilizar um parâmetro de regularização a fim de evitar os super ajustamento ao conjunto de treinamento, balançeando a complexidade da hipótese.

Para seleção dos parâmetros testou-se, através de busca em grid, as 3 hipóteses, com parâmetro \(\lambda\) = 0, ou seja, sem regularização, e com a regularização variando de \(10^0\) a \(10^3\) 

\subsection{Redes Neurais Artificiais}

As Redes Neurais Artificias, utilizadas foram os Perceptrons Multi-camadas, que consistituem uma série de camadas massivamente conectadas de regressores logísticos, portanto, o método consiste em ajustar matrizes de coeficientes \(\theta\) a fim de minimizar o erro de classificação das amostras.

Entre os parâmetros a serem ajustados, existe a taxa de aprendizagem \(\alpha\), o número de camadas o número de neurônios de cada camada.

\subsection{Máquinas de vetores de suporte}

O SVM foi implementado utilizando-se a biblioteca LIBSVM colocar url, 

Os parâmetros incluem a seleção do kernel, dos coeficientes C (para os kernel linear, radial e polinomial) e \(\gamma\) (para os kernel radial e polinomial)

\subsection{Naive Bayes}

O método Naive-Bayes se baseia nas probabilidades de ocorrência de cada classe, e de cada atributo individualmente sabendo a classe em que o mesmo se encontra. O métodos Naive-Bayes se baseia apenas nas probabilidades, portanto não possuí parametros a serem ajustados.

